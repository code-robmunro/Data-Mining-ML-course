# CS6220_DataMining Grades


### Assignment 0 [100/100]
```
Well done!
```
<br>

### Assignment 1 [100/100]
```
1. 10/10
2. 10/10
3. 10/10 
4. 20/20 
5. 30/30 
6. 20/20
```
<br>

### Assignment 2 [98/100]
```
1.1 15/15
1.2 20/20
2.1 15/15
2.2 20/20
3.1 10/10
3.2 08/10 [Isn't the pair of sepal length and petal width have most significantly different medians?]
3.3 10/10
```
<br>

### Assignment 3 [90/100]
```
1.0 10/25 [implement PCA incorrectly, the way you did is ploting the original image set, not the images after processed by PCA]
2.1 15/15
2.2 10/10 
3.0 50/50 
```
<br>

### Assignment 4 [100/100]
```
1. 40/40
2. 40/40
3. 20/20
```
<br>

### Assignment 5 [95/100]
```
1.0 30/30
2.0 30/30
3.0 35/40 [It might be tempting to say that the results for SSQ are immediately better, but there is some degree of subjectivity in choosing K based on that metric. To a different observer, perhaps a value of k=2 or k=4 would seem more ideal, and one could argue that for this particular dataset, using this method is no better than using gap statistic.]
```
<br>


### Midterm [84/100]
```
1 | 10/10

2 | 6/10 -- A horizontal line would capture most of the variance and represent the first component, with a line perpendicular to that being the second.

3 | 10/10 

4 | 10/10

5 | 8/10 -- Cluster #3 should be the result of adding the instance in the middle to cluster #1 

6 | 10/10 

7 | 7/10 -- You're perfectly correct on your interpretation of what would happen with 5-nearest neighbors here, but the other two classifiers would actually suffer from the fact that, in their simplest form, they would produce linear decision boudaries which would not work well on this dataset. Take a look at the provided solution for more details.

8 | 4/10 -- Here, when k=1, each point will receive the correct class as the closest point to it will be itself, which would hence produce the correct class. When k=3, 1 point would be incorrectly classified. See the provided solution for more details.

9 | 6/10 -- You had most things right and the prediction you generated makes sense based on the setup you had. P(UG) should have been set to 0.8, which is one of the things that seems to have thrown your calculations off. Take a closer look at the provided solutions for other details.

10 | 10/10 -- Good observations! The one thing that I would add here is that you could use the actual labels provided to you to evaluate how well your clustering algorithm actually grouped the data points. That is... data points belonging to the same class should ideally be grouped on the same cluster.

Extra Credit | 3/10 -- Your initial idea of using association rules would probably work quite well here barring some challenges you may run into related to data sparcity. Another idea would be to use collaborative filtering.

Speaker Spotlight Series Extra Credit | 0/5

TOTAL: 84/100 
```
<br>

<br>

### Project Proposal [100/100]
```
Feedback provided on slack.
```

<br>