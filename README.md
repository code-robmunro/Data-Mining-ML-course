# CS6220_DataMining Grades


### Assignment 0 [100/100]
```
Well done!
```

<br>

### Assignment 1 [100/100]
```
1. 10/10
2. 10/10
3. 10/10 
4. 20/20 
5. 30/30 
6. 20/20
```

<br>

### Assignment 2 [98/100]
```
1.1 15/15
1.2 20/20
2.1 15/15
2.2 20/20
3.1 10/10
3.2 08/10 [Isn't the pair of sepal length and petal width have most significantly different medians?]
3.3 10/10
```

<br>

### Assignment 3 [90/100]
```
1.0 10/25 [implement PCA incorrectly, the way you did is ploting the original image set, not the images after processed by PCA]
2.1 15/15
2.2 10/10 
3.0 50/50 
```

<br>

### Assignment 4 [100/100]
```
1. 40/40
2. 40/40
3. 20/20
```

<br>

### Assignment 5 [95/100]
```
1.0 30/30
2.0 30/30
3.0 35/40 [It might be tempting to say that the results for SSQ are immediately better, but there is some degree of subjectivity in choosing K based on that metric. To a different observer, perhaps a value of k=2 or k=4 would seem more ideal, and one could argue that for this particular dataset, using this method is no better than using gap statistic.]
```

<br>

### Midterm [84/100]
```
1 | 10/10

2 | 6/10 -- A horizontal line would capture most of the variance and represent the first component, with a line perpendicular to that being the second.

3 | 10/10 

4 | 10/10

5 | 8/10 -- Cluster #3 should be the result of adding the instance in the middle to cluster #1 

6 | 10/10 

7 | 7/10 -- You're perfectly correct on your interpretation of what would happen with 5-nearest neighbors here, but the other two classifiers would actually suffer from the fact that, in their simplest form, they would produce linear decision boudaries which would not work well on this dataset. Take a look at the provided solution for more details.

8 | 4/10 -- Here, when k=1, each point will receive the correct class as the closest point to it will be itself, which would hence produce the correct class. When k=3, 1 point would be incorrectly classified. See the provided solution for more details.

9 | 6/10 -- You had most things right and the prediction you generated makes sense based on the setup you had. P(UG) should have been set to 0.8, which is one of the things that seems to have thrown your calculations off. Take a closer look at the provided solutions for other details.

10 | 10/10 -- Good observations! The one thing that I would add here is that you could use the actual labels provided to you to evaluate how well your clustering algorithm actually grouped the data points. That is... data points belonging to the same class should ideally be grouped on the same cluster.

Extra Credit | 3/10 -- Your initial idea of using association rules would probably work quite well here barring some challenges you may run into related to data sparcity. Another idea would be to use collaborative filtering.

Speaker Spotlight Series Extra Credit | 0/5

TOTAL: 84/100 
```

<br>

### Assignment 6 [95/100]
```
Implementation [55/60] lack of multiple features implementation
Question 1 [20/20]
Question 2 [20/20]
```

<br>

### Assignment 7 [100/100]
```
1. 20/20
2. 20/20
3. 20/20
4. 20/20
5. 20/20
```

<br>

### Project Proposal [100/100]
```
Feedback provided on slack.
```

<br>

### Project Presentation [95/100]
```
Great presentation using datasets that are quite interesting and that elicit lots of thoughts and questions. I hope that through the work that you're doing for this project you are learning how these ML techniques can be readily applied to solve real problems such as the one you are tackling. Your presentation went quite a bit overtime but that's fine given the amount of content you covered. I did have a few minor comments regarding the practical usefulness of some of the use-cases that you suggested for your models during the presentation, and if you have a chance to address those, it'd be cool to have that covered in your final paper. As you wrap that up and think about how someone could use what you're developing, make sure to highlight that information. If during the process you ran into other ideas that you won't have time to work on but that you think would be great extensions to what you did, make sure to list those as well. Other than these minor comments, you guys are on the right track. Great job so far!
```

<br>

### Project Paper [95/100]
```
* Well written and organized paper!
* Some of the images are a bit low-res and hard to read.
* It seems that you learned quite a bit in the process, and your conclusion in particular does a good job describing all the challenges you ran into. This serves as a great example of how amazingly useful machine learning can be, but it also highlights how difficult it is to formulate the proper questions, collect the correct data, and choose the right models to tackle these questions.
```

<br>
